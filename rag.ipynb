{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# ğŸ“š LangChain ile Retrieval Augmented Generation'a GiriÅŸ ğŸ¦œğŸ”—\n",
    "\n",
    "Bu not defterinde LangChain kullanarak Retrieval Augmented Generation'Ä± nasÄ±l kullanacaÄŸÄ±nÄ±zÄ± Ã¶ÄŸreneceksiniz.\n",
    "\n",
    "Kendi belgelerimiz hakkÄ±nda sorular sormak iÃ§in bir LLM kullanacaÄŸÄ±z!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## âš™ï¸ Kurulum\n",
    "\n",
    "ğŸ‘‰ Temel kÃ¼tÃ¼phaneleri iÃ§e aktarmak iÃ§in aÅŸaÄŸÄ±daki hÃ¼creyi Ã§alÄ±ÅŸtÄ±rÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(\"PYTHON:\", sys.executable)\n",
    "load_dotenv()\n",
    "print(\"KEY?\", bool(os.getenv(\"GOOGLE_API_KEY\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "import chromadb\n",
    "import pypdf\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "print(\"imports OK âœ…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from pprint import pprint\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "ğŸ‘‰ API anahtarÄ±mÄ±zÄ± tekrar yÃ¼klemek iÃ§in aÅŸaÄŸÄ±daki hÃ¼creyi Ã§alÄ±ÅŸtÄ±rÄ±n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## ğŸ“š Neden RAG?\n",
    "\n",
    "Bir LLM kendi baÅŸÄ±na, Ã¶ÄŸrendiÄŸi her ÅŸey hakkÄ±nda sorulara yanÄ±t verebilir.\n",
    "\n",
    "Bunun birkaÃ§ dezavantajÄ± vardÄ±r:\n",
    "- EÄŸitim verileri geÃ§miÅŸten gelir ve en son verilerle gÃ¼ncellenmez.\n",
    "- Sadece eÄŸitim aldÄ±ÄŸÄ± verileri bilir.\n",
    "\n",
    "Bir LLM'yi kendi verilerimizle Ã§alÄ±ÅŸmasÄ± iÃ§in kullanmak istiyoruz. Ä°ÅŸte bu noktada RAG (Retrieval-Augmented Generation) devreye girer.\n",
    "\n",
    "1. **Retrieval-Augmented Generation (RAG)**, gerÃ§ek doÄŸruluÄŸu artÄ±rmak iÃ§in bir dil modelini belge alÄ±cÄ± ile birleÅŸtirir.\n",
    "2. **Ä°lgili dÄ±ÅŸ belgeleri alÄ±r** (Ã¶rneÄŸin, bilgi tabanÄ±ndan) yanÄ±tlar Ã¼retmeden Ã¶nce.\n",
    "3. **Dil modeli hem istemi hem de alÄ±nan baÄŸlamÄ± kullanarak** daha bilgili ve temelli Ã§Ä±ktÄ±lar Ã¼retir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## ğŸ‡ªğŸ‡º BaÄŸlam\n",
    "\n",
    "Bu meydan okumada, Avrupa Parlamentosu'ndan belgelerle Ã§alÄ±ÅŸacaÄŸÄ±z.\n",
    "\n",
    "Bir gazeteci olduÄŸunuzu ve Avrupa Parlamentosu'nun genel kurul oturumlarÄ± sÄ±rasÄ±nda belirli bir konu hakkÄ±nda neler sÃ¶ylendiÄŸini Ã¶ÄŸrenmek istediÄŸinizi dÃ¼ÅŸÃ¼nÃ¼n. Bu oturumlar yÄ±lda 12 kez Strasbourg'da gerÃ§ekleÅŸir ve 4 gÃ¼n sÃ¼rer. OturumlarÄ±n transkriptleri EP'nin web sitesinde mevcuttur.\n",
    "\n",
    "Kesinlikle tÃ¼m bu transkriptleri karÄ±ÅŸtÄ±rmak istemezsiniz. O halde, hayatÄ±mÄ±zÄ± kolaylaÅŸtÄ±rmak iÃ§in RAG'Ä± kullanalÄ±m!\n",
    "\n",
    "Bu, her zaman test etmek iÃ§in yepyeni veriler alabileceÄŸimiz iÃ§in Ã§alÄ±ÅŸmak Ã¼zere iyi verilerdir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## ğŸ“˜ Verileri alalÄ±m\n",
    "\n",
    "1. [EP'nin web sitesine](https://www.europarl.europa.eu/plenary/en/debates-video.html) gidin. \n",
    "1. Bu sizi en son genel kurul oturumuna yÃ¶nlendirecektir.\n",
    "1. Ä°lk tarihin altÄ±nda, \"â–¶ï¸ Verbatim reports HTML\" bÃ¶lÃ¼mÃ¼nde `HTML`'e tÄ±klayÄ±n.\n",
    "1. SayfanÄ±n sonuna kaydÄ±rÄ±n ve alttaki PDF dosyasÄ±nÄ± indirin.\n",
    "1. DosyayÄ± `data` klasÃ¶rÃ¼ne kaydedin.\n",
    "\n",
    "Bir belgeyle baÅŸlayacaÄŸÄ±z, ancak daha sonrasÄ± iÃ§in diÄŸer birkaÃ§ gÃ¼nÃ¼n aynÄ±sÄ±nÄ± ÅŸimdiden indirebilirsiniz.\n",
    "\n",
    "Belgeye bir gÃ¶z atÄ±n. KaÃ§ sayfasÄ± var? Belge hakkÄ±nda bir fikir edinmek iÃ§in hÄ±zlÄ±ca belgede gezinin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## ğŸ”¢ Belgeleri gÃ¶mme\n",
    "\n",
    "Belgeleri gÃ¶mmek, tÃ¼m belgeleri veya belge parÃ§alarÄ±nÄ± vektÃ¶rlere Ã§evirmek anlamÄ±na gelir.\n",
    "\n",
    "LangChainğŸ¦œğŸ”— yine Ã§ok yardÄ±mcÄ± olacak.\n",
    "\n",
    "Bir gÃ¶mme aracÄ± (embedder) baÅŸlatalÄ±m ve deneyelim. LLM olarak Gemini kullandÄ±ÄŸÄ±mÄ±z iÃ§in, Google'Ä±n metin gÃ¶mme araÃ§larÄ±nda kalalÄ±m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "ğŸ‘‰ Basit bir metin parÃ§asÄ±nÄ± gÃ¶mmek iÃ§in gÃ¶mme aracÄ±nÄ±n `.embed_query()` metodunu deneyin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "tags": [
     "steps"
    ]
   },
   "outputs": [],
   "source": [
    "# Embed a text like \"What is the capital of France?\" and save it to a variable `sample_embedding`\n",
    "\n",
    "sample_text = \"What is the capital of France?\"\n",
    "sample_embedding = embeddings.embed_query(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "ğŸ‘‰ Bu `sample_embedding`'i keÅŸfetmek iÃ§in zaman ayÄ±rÄ±n. NasÄ±l gÃ¶rÃ¼nÃ¼yor? Tipi nedir? GÃ¶mme boyutu nedir?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "type(sample_embedding), len(sample_embedding), sample_embedding[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "math.sqrt(sum(x * x for x in sample_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## ğŸ’¾ PDF'den gerÃ§ek verilerimizi yÃ¼kle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "ArtÄ±k bir gÃ¶mmenin nasÄ±l gÃ¶rÃ¼ndÃ¼ÄŸÃ¼nÃ¼ biliyoruz, gerÃ§ek verilerimizle Ã§alÄ±ÅŸmanÄ±n zamanÄ± geldi.\n",
    "\n",
    "ğŸ‘‰ [LangChain belgelerine](https://docs.langchain.com/oss/python/integrations/document_loaders/index#pdfs) gidin ve PyPDF kullanarak bir PDF'yi nasÄ±l yÃ¼kleyebileceÄŸinizi Ã¶ÄŸrenin.\n",
    "\n",
    "ğŸ‘‰ Sonra devam edin ve daha Ã¶nce indirdiÄŸiniz PDF'lerden birini yÃ¼kleyin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"data/CRE-10-2026-01-27_EN.pdf\"\n",
    "loader = PyPDFLoader(file_path\n",
    "                    )\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "ğŸ‘‰ `pages`'i keÅŸfedin:\n",
    "- Veri tipi nedir?\n",
    "- KaÃ§ sayfanÄ±z var?\n",
    "- Bir sayfanÄ±n tipi nedir?\n",
    "- Bir sayfanÄ±n iÃ§eriÄŸine nasÄ±l eriÅŸebilirsiniz?\n",
    "- Tam belgenin kaÃ§ karakteri var?\n",
    "- Bir sayfanÄ±n `metadata`'sÄ±nda neler var?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# Veri tipi\n",
    "type(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KaÃ§ sayfa var?\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bir sayfanÄ±n tipi\n",
    "type(pages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bir sayfanÄ±n iÃ§eriÄŸi (ilk 500 karakter)\n",
    "pages[0].page_content[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TÃ¼m belgenin toplam karakter sayÄ±sÄ±\n",
    "sum(len(page.page_content) for page in pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bir sayfanÄ±n metadata'sÄ±\n",
    "pages[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## âœ‚ï¸ Verilerimizi bÃ¶l\n",
    "\n",
    "Tam belgemiz gÃ¶mÃ¼lmek iÃ§in Ã§ok uzun. Metin gÃ¶mme aracÄ±mÄ±z 2.048 tokena kadar giriÅŸ alabilir. Gemini modelleri iÃ§in bu yaklaÅŸÄ±k 8.196 karakterdir (token baÅŸÄ±na 4 karakter).\n",
    "\n",
    "Bu yÃ¼zden belgemizi daha kÃ¼Ã§Ã¼k parÃ§alara bÃ¶lmek istiyoruz.\n",
    "\n",
    "Zaten Ã§alÄ±ÅŸabileceÄŸimiz bir dizi sayfamÄ±z var. Ama sayfa sonlarÄ± biraz keyfi: genellikle cÃ¼mlenin ortasÄ±nda gÃ¶rÃ¼nÃ¼rler.\n",
    "\n",
    "AyrÄ±ca, sayfalar arasÄ±nda Ã¶rtÃ¼ÅŸme yoktur. Bu yÃ¼zden bir sayfanÄ±n ilk satÄ±rÄ± Ã¶nceki tÃ¼m baÄŸlamÄ± kaÃ§Ä±rÄ±r. Tam metni biraz Ã¶rtÃ¼ÅŸmeyle bÃ¶lmek daha iyidir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Ä°lk olarak, PDF'yi tekrar yÃ¼kleyeceÄŸiz, bu sefer bÃ¶lmeden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(file_path, mode='single')\n",
    "pdf = loader.load()\n",
    "pdf_text = pdf[0].page_content\n",
    "len(pdf_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "ArtÄ±k tÃ¼m PDF'imizi tek bir belge olarak aldÄ±ÄŸÄ±mÄ±za gÃ¶re, onu daha akÄ±llÄ± bir ÅŸekilde parÃ§alara bÃ¶lebiliriz.\n",
    "\n",
    "ğŸ‘‰ Yine, [\"Ã–zyinelemeli olarak bÃ¶lme\" konusundaki LangChain belgelerine](https://docs.langchain.com/oss/python/integrations/splitters/recursive_text_splitter) gidin ve `pdf` _belgelerimizi_ parÃ§alara (LangChain'de `documents` olarak adlandÄ±rÄ±lÄ±r) nasÄ±l bÃ¶leceÄŸinizi Ã¶ÄŸrenin.\n",
    "\n",
    "2_000 karakter (bizim durumumuzda yaklaÅŸÄ±k yarÄ±m sayfa) parÃ§alara 400 Ã¶rtÃ¼ÅŸmeyle bÃ¶lÃ¼n. Ä°sterseniz diÄŸer deÄŸerlerle deneyebilirsiniz.\n",
    "\n",
    "`RecursiveCharacterTextSplitter`'Ä±n `.split_documents()` metodunu kullanÄ±n: bu metod giriÅŸ olarak bir belge alÄ±r ve bÃ¶lÃ¼nmÃ¼ÅŸ belgeler Ã§Ä±ktÄ±sÄ± verir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"data/CRE-10-2026-01-27_EN.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(file_path, mode=\"single\")\n",
    "pdf = loader.load()\n",
    "\n",
    "pdf_text = pdf[0].page_content\n",
    "len(pdf_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "ğŸ‘‰ `all_splits`'i inceleyin:\n",
    "- Veri tipi nedir?\n",
    "- KaÃ§ bÃ¶lÃ¼mÃ¼nÃ¼z var?\n",
    "- Bir bÃ¶lÃ¼mÃ¼n tipi nedir?\n",
    "- Bir bÃ¶lÃ¼mÃ¼n iÃ§eriÄŸine nasÄ±l eriÅŸebilirsiniz?\n",
    "- Åimdi toplamda kaÃ§ karakterimiz var?\n",
    "- Bir bÃ¶lÃ¼mÃ¼n `metadata`'sÄ±nda neler var?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Tek bir Document oluÅŸtur (metadata ekleyelim ki source kaybolmasÄ±n)\n",
    "doc = Document(\n",
    "    page_content=pdf_text,\n",
    "    metadata={\n",
    "        \"source\": file_path,\n",
    "        \"doc_name\": \"CRE-10-2026-01-27_EN\",\n",
    "    },\n",
    ")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=400,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],  # en doÄŸal bÃ¶lme sÄ±rasÄ±\n",
    ")\n",
    "\n",
    "all_splits = text_splitter.split_documents([doc])\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri tipi nedir?\n",
    "type(all_splits)\n",
    "\n",
    "# KaÃ§ bÃ¶lÃ¼m var?\n",
    "len(all_splits)\n",
    "\n",
    "# Bir bÃ¶lÃ¼mÃ¼n tipi nedir?\n",
    "type(all_splits[0])\n",
    "\n",
    "# Bir bÃ¶lÃ¼mÃ¼n iÃ§eriÄŸine nasÄ±l eriÅŸilir? (ilk 300 karakter)\n",
    "all_splits[0].page_content[:300]\n",
    "\n",
    "# Åimdi toplamda kaÃ§ karakterimiz var?\n",
    "sum(len(d.page_content) for d in all_splits)\n",
    "\n",
    "# Bir bÃ¶lÃ¼mÃ¼n metadata'sÄ±nda neler var?\n",
    "all_splits[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## ğŸ—„ï¸ Her ÅŸeyi bir araya getir: belgelerimizi gÃ¶mme ve vektÃ¶r deposunda sakla\n",
    "\n",
    "Elimizde ÅŸunlar var:\n",
    "- Bir gÃ¶mme aracÄ±\n",
    "- Veriyi yÃ¼klemek iÃ§in bir yÃ¼kleyici\n",
    "- Belgemizi belgelere bÃ¶lmek iÃ§in bir metin bÃ¶lÃ¼cÃ¼\n",
    "\n",
    "Neyi kaÃ§Ä±rÄ±yoruz?\n",
    "\n",
    "Belgelerimizi gÃ¶mebiliriz, ama onlarÄ± bir yerde saklamak istiyoruz. Ä°ÅŸte burada vektÃ¶r deposu devreye girer: ÅŸunlarÄ± saklamamÄ±za olanak saÄŸlar:\n",
    "- belgeyi (parÃ§ayÄ±),\n",
    "- onun gÃ¶mmesini,\n",
    "- meta verilerini.\n",
    "\n",
    "Sonraki adÄ±mda belgeleri verimli bir ÅŸekilde alabilecek olacaÄŸÄ±z.\n",
    "\n",
    "ğŸ‘‰ Bir `InMemoryVectorStore` nasÄ±l oluÅŸturabileceÄŸinizi gÃ¶rmek iÃ§in [\"VektÃ¶r depolarÄ±\" Ã¼zerine LangChain belgelerini](https://docs.langchain.com/oss/python/langchain/knowledge-base#3-vector-stores) kontrol edin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "tags": [
     "steps"
    ]
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "# Create an in-memory vector store using the embedder `embeddings` we created earlier\n",
    "\n",
    "vector_store = InMemoryVectorStore(embedding=embeddings)\n",
    "\n",
    "# Add the `all_splits` to the vector store and store the result in a variable called `document_ids`\n",
    "\n",
    "document_ids = vector_store.add_documents(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "tags": [
     "steps"
    ]
   },
   "outputs": [],
   "source": [
    "# Have a look at the first 3 document IDs\n",
    "\n",
    "document_ids[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "tags": [
     "steps"
    ]
   },
   "outputs": [],
   "source": [
    "# Use the vector store's `get_by_ids` method. You have to give it a list of document IDs.\n",
    "\n",
    "docs = vector_store.get_by_ids(document_ids[:1])\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "ğŸ‘‰ Bir vektÃ¶r deposundaki belgenin iÃ§eriÄŸine ve meta verilerine nasÄ±l eriÅŸebilirsiniz?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "docs[0].page_content[:300], docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## ğŸ” Benzer belgeleri almak iÃ§in vektÃ¶r deposunu kullan\n",
    "\n",
    "ArtÄ±k belgeleri gÃ¶mleÄŸe Ã§evirdiÄŸimize gÃ¶re, benzer belgeleri almak iÃ§in vektÃ¶r deposunu kullanabiliriz.\n",
    "\n",
    "ğŸ‘‰ Bunun nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶rmek iÃ§in [\"VektÃ¶r depolarÄ±\" Ã¼zerine LangChain belgelerini](https://docs.langchain.com/oss/python/langchain/knowledge-base#3-vector-stores) kontrol edin.\n",
    "\n",
    "Bir sorgu kullanÄ±n, Ã¶rneÄŸin \"TarÄ±m politikasÄ± Ã¼zerine tartÄ±ÅŸmayÄ± Ã¶zetle.\", ve en benzer belgeleri bulun. AyrÄ±ca alÄ±nacak belge sayÄ±sÄ±nÄ± da belirtebilirsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "tags": [
     "steps"
    ]
   },
   "outputs": [],
   "source": [
    "# Save your question into a variable called `query`\n",
    "\n",
    "query = \"What were the main topics discussed in this session? Summarize in bullet points.\"\n",
    "\n",
    "# Use the vector store to find similar documents to the query. Store the result in a variable called `retrieved_docs`\n",
    "\n",
    "k = 4  # istersen 3-8 arasÄ± oynat\n",
    "retrieved_docs = vector_store.similarity_search(query, k=k)\n",
    "retrieved_docs\n",
    "\n",
    "[(d.metadata, d.page_content[:200]) for d in retrieved_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "Bu, RAG'Ä±n sÃ¶zde \"Alma\" (Retrieval) kÄ±smÄ±nÄ± tamamlar: artÄ±k sorgumuza en benzer belgeleri bulabiliriz.\n",
    "\n",
    "Ã‡alÄ±ÅŸmanÄ±n Ã§oÄŸu artÄ±k tamamlandÄ±!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "## ğŸ’¬ Sorumuza bir cevap Ã¼ret\n",
    "\n",
    "Åimdiye kadar benzer belgeleri almamÄ±zÄ± saÄŸlamak iÃ§in sadece bir **gÃ¶mme modeli** kullandÄ±k.\n",
    "\n",
    "Åimdi, sorumuzla bir cevap almak iÃ§in Ã¼retici bir LLM kullanacaÄŸÄ±z: ona aldÄ±ÄŸÄ±mÄ±z belgeler ve sorumuzla besleyeceÄŸiz.\n",
    "\n",
    "Bunu yapmanÄ±n en temel yolu tÃ¼m girdilerimizi birbirine baÄŸlamak, sorumuzla eklemek ve sonucu gÃ¶rmek olacaktÄ±r.\n",
    "\n",
    "Bir deneyelim.\n",
    "\n",
    "ğŸ‘‰ Ä°lk olarak Ã¶nceki meydan okumalarda olduÄŸu gibi bir LLM baÅŸlatÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "Sonra temel bir istem oluÅŸturun:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the main topics discussed in this session?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "ğŸ‘‰ Åimdi istemi kullanÄ±n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "retrieved_docs = vector_store.similarity_search(query, k=4)\n",
    "\n",
    "# Ä°stersen hÄ±zlÄ± kontrol:\n",
    "len(retrieved_docs), type(retrieved_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "Bu fena deÄŸil, ama modele daha fazla rehberlik vererek daha kapsamlÄ± bir istem yazarak daha iyisini yapabiliriz.\n",
    "\n",
    "Bunu yapan ilk kiÅŸiler biz deÄŸilmiÅŸiz ve LangChain'in bizim iÃ§in Ã¶nceden hazÄ±rlanmÄ±ÅŸ istem kÃ¼tÃ¼phanesi var.\n",
    "\n",
    "ğŸ‘‰ AÅŸaÄŸÄ±daki hÃ¼creyi Ã§alÄ±ÅŸtÄ±rÄ±n ve nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± anlamaya Ã§alÄ±ÅŸÄ±n. (LangSmithMissingAPIKeyWarning hakkÄ±nda bir uyarÄ± alacaksÄ±nÄ±z, bunu gÃ¶rmezden gelebilirsiniz.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_CHARS = 1500  # artÄ±rÄ±rsan kalite artabilir ama token/maliyet de artar\n",
    "\n",
    "context = \"\\n\\n\".join(\n",
    "    doc.page_content[:CHUNK_CHARS] for doc in retrieved_docs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "LangChain'in bizim iÃ§in nasÄ±l daha kesin bir istem oluÅŸturduÄŸunu gÃ¶rÃ¼yor musunuz? Bunu RAG'Ä±mÄ±z iÃ§in kullanalÄ±m!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are a question-answering assistant.\n",
    "Answer using ONLY the CONTEXT below.\n",
    "\n",
    "Return:\n",
    "1) Answer (short)\n",
    "2) Evidence: quote 1-2 short snippets from the context that support the answer.\n",
    "\n",
    "If the answer is not in the context, say: \"I don't know based on the provided context.\"\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{query}\n",
    "\n",
    "FORMAT:\n",
    "Answer: ...\n",
    "Evidence:\n",
    "- \"...\"\n",
    "- \"...\"\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_response = llm.invoke(prompt)\n",
    "print(final_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "ğŸ‘‰ Ä°lk olarak, tÃ¼m alÄ±nan belgeleri iki yeni satÄ±rla ayrÄ±lmÄ±ÅŸ tek bir uzun dizgiye birleÅŸtirin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "ğŸ‘‰ Sonra, sorgunuz ve alÄ±nan belgelerden baÅŸlayarak bir `prompt` oluÅŸturun. YukarÄ±daki Ã¶rneÄŸe bakmayÄ± unutmayÄ±n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "ğŸ‘‰ Son olarak az Ã¶nce oluÅŸturduÄŸumuz `the_prompt` ile LLM modelini kullanÄ±n:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "ğŸ‰ Ä°lk RAG'Ä±mÄ±zÄ± tamamladÄ±k: LLM kendisine saÄŸladÄ±ÄŸÄ±mÄ±z belgelerde ***temelli*** metin Ã¼retti."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "## ğŸ’¾ GÃ¶mmelerimizi kalÄ±cÄ± hale getir\n",
    "\n",
    "Åimdiye kadar bellekte vektÃ¶r deposuyla Ã§alÄ±ÅŸtÄ±k. Bu yÃ¼zden not defterinizi kapattÄ±ÄŸÄ±nÄ±zda, tÃ¼m gÃ¶mmeleri de kaybedeceksiniz.\n",
    "\n",
    "âš ï¸ Bu gÃ¶mmelerin saÄŸlayÄ±cÄ±nÄ±zÄ±n platformunda, bu durumda Google'Ä±n makinelerinde Ã§alÄ±ÅŸan modeller tarafÄ±ndan Ã¼retildiÄŸini unutmayÄ±n. Ve bedava Ã§alÄ±ÅŸmazlar. ğŸ’°\n",
    "\n",
    "Bunun gibi bir, nispeten kÃ¼Ã§Ã¼k belge iÃ§in maliyet dÃ¼ÅŸÃ¼ktÃ¼r, ama hÄ±zla artar. Åimdiye kadar sadece bir gÃ¼nÃ¼n transkriptleriyle Ã§alÄ±ÅŸtÄ±k. Oturum baÅŸÄ±na 3 tane daha, yÄ±lda 12 oturum, birden fazla yÄ±l var...\n",
    "\n",
    "Bunu Ã§Ã¶zmek iÃ§in sadece vektÃ¶r depomuzla kalÄ±cÄ± bir taneyi deÄŸiÅŸtireceÄŸiz. Bu LangChain'in avantajÄ±dÄ±r: bileÅŸenleri deÄŸiÅŸtirmek Ã§ok kolay.\n",
    "\n",
    "Bellekteki vektÃ¶r depomuz deneme iÃ§in harikaydÄ±, ÅŸimdi baÅŸka bir taneyle deÄŸiÅŸtireceÄŸiz. Ã‡ok popÃ¼ler bir vektÃ¶r deposu olan [Chroma](https://www.trychroma.com/)'yÄ± kullanacaÄŸÄ±z. Bunu yerel olarak Ã§alÄ±ÅŸtÄ±rabilir ve LangChain aracÄ±lÄ±ÄŸÄ±yla kullanabiliriz.\n",
    "\n",
    "TÃ¼m akÄ±ÅŸÄ±mÄ±zÄ± yeniden oluÅŸturacaÄŸÄ±z. Her ÅŸeyi birkaÃ§ kod hÃ¼cresinde tekrar bir araya getirmeye Ã§alÄ±ÅŸmak iyi bir alÄ±ÅŸtÄ±rmadÄ±r. AynÄ± zamanda her ÅŸeyi yeniden kullanÄ±labilir koda dÃ¶nÃ¼ÅŸtÃ¼receÄŸiz.\n",
    "\n",
    "Sonunda iki fonksiyon istiyoruz:\n",
    "\n",
    "1. `embed_and_store()`: BaÅŸka bir oturumun transkriptini vektÃ¶r veritabanÄ±mÄ±za ekle, bÃ¶ylece alacaÄŸÄ±mÄ±z daha fazla veri olsun.\n",
    "2. `answer()`: VektÃ¶r depomuzla farklÄ± sorularla sorgula."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "#### 1. Bir Chroma vektÃ¶r deposu baÅŸlat\n",
    "\n",
    "ğŸ‘‰ **Veri kalÄ±cÄ±lÄ±ÄŸÄ±yla** (yani verileri diskteki bir dizinde saklayarak) Chroma vektÃ¶r deposunun nasÄ±l oluÅŸturulacaÄŸÄ±nÄ± gÃ¶rmek iÃ§in [LangChain'in belgelerine](https://python.langchain.com/docs/integrations/vectorstores/chroma/) bakÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -q install -U langchain langchain-core langchain-community langchain-google-genai chromadb pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import date\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "# Embedding (vektÃ¶r oluÅŸturmak iÃ§in)\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/text-embedding-004\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "PERSIST_DIR = \"chroma_store\"          # klasÃ¶r adÄ± (proje iÃ§inde oluÅŸur)\n",
    "COLLECTION_NAME = \"sessions\"          # koleksiyon adÄ±\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    persist_directory=PERSIST_DIR,\n",
    "    embedding_function=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "#### 2. `embed_and_store()` oluÅŸtur\n",
    "\n",
    "ğŸ‘‰ Bu fonksiyon iÃ§in kodu tamamlayÄ±n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_and_store(\n",
    "    file_path: str,\n",
    "    vector_store: Chroma,\n",
    "    session_date: str | None = None,\n",
    "    chunk_size: int = 1200,\n",
    "    chunk_overlap: int = 150,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load a PDF, split into chunks, add metadata, and store in Chroma.\n",
    "    Returns: list of stored document ids (or empty list if none).\n",
    "    \"\"\"\n",
    "    if session_date is None:\n",
    "        session_date = str(date.today())\n",
    "\n",
    "    # 1) Load PDF pages\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    docs = loader.load()  # list[Document] (page-level)\n",
    "\n",
    "    # 2) Split into chunks\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    )\n",
    "    splits = splitter.split_documents(docs)\n",
    "\n",
    "    # 3) Add metadata\n",
    "    for d in splits:\n",
    "        d.metadata[\"session_date\"] = session_date\n",
    "        d.metadata[\"source\"] = d.metadata.get(\"source\", file_path)\n",
    "\n",
    "    # 4) Store\n",
    "    ids = vector_store.add_documents(splits)\n",
    "\n",
    "    # Chroma Ã§oÄŸu zaman otomatik persist eder ama garanti olsun:\n",
    "    vector_store.persist()\n",
    "\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "ğŸ‘‰ Fonksiyonunuzu bir dosya veya hatta iki dosyayla deneyin:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "#### 3. `answer()` oluÅŸtur\n",
    "\n",
    "ğŸ‘‰ Bu fonksiyon iÃ§in kodu tamamlayÄ±n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are a question-answering assistant.\n",
    "Answer using ONLY the CONTEXT below.\n",
    "\n",
    "Return:\n",
    "1) Answer (short)\n",
    "2) Evidence: quote 1-2 short snippets from the context that support the answer.\n",
    "\n",
    "If the answer is not in the context, say: \"I don't know based on the provided context.\"\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{query}\n",
    "\n",
    "FORMAT:\n",
    "Answer: ...\n",
    "Evidence:\n",
    "- \"...\"\n",
    "- \"...\"\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(\n",
    "    query: str,\n",
    "    vector_store: Chroma,\n",
    "    llm: ChatGoogleGenerativeAI,\n",
    "    k: int = 6,\n",
    "    max_chars_per_doc: int = 1500,\n",
    ") -> str:\n",
    "    \"\"\"Retrieve docs from vector store and answer using the LLM.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=k)\n",
    "\n",
    "    context = \"\\n\\n\".join(d.page_content[:max_chars_per_doc] for d in retrieved_docs)\n",
    "\n",
    "    prompt = PROMPT_TEMPLATE.format(context=context, query=query)\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "ğŸ‘‰ Fonksiyonunuzu beÄŸendiÄŸiniz bir sorguyla deneyin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "ids = embed_and_store(\n",
    "    file_path=\"data/CRE-10-2026-01-27_EN.pdf\",\n",
    "    vector_store=vector_store,\n",
    "    session_date=\"2026-01-27\",\n",
    ")\n",
    "len(ids), ids[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the main topics discussed in this session?\"\n",
    "print(answer(query, vector_store, llm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "ğŸ Tebrikler! ArtÄ±k LangChain kullanarak RAG'da ustalaÅŸtÄ±nÄ±z ve vektÃ¶r deponuza daha fazla belge eklemek ve onu sorgulamak iÃ§in yeniden kullanÄ±labilir fonksiyonlar yapmayÄ± Ã¶ÄŸrendiniz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "## [Ä°steÄŸe BaÄŸlÄ±] Meta veri ekleme\n",
    "\n",
    "KurduÄŸumuz RAG, vektÃ¶r deposundaki tÃ¼m belgeleri sorgular. Orada birden fazla yÄ±lÄ±n bilgisinin olduÄŸunu dÃ¼ÅŸÃ¼nÃ¼n. YÄ±llara veya tarihlere gÃ¶re filtreyebilsek kullanÄ±ÅŸlÄ± olurdu, deÄŸil mi?\n",
    "\n",
    "Bunu nasÄ±l yaparÄ±z? VektÃ¶r deposundaki belgelerin meta veri iÃ§erdiÄŸini unutmayÄ±n. EÄŸer tarihi ekleyebilseydik, daha sonra filtrelemek iÃ§in kullanabilirdik.\n",
    "\n",
    "Ä°pucu: Meta verilerinizi pipeline'Ä±nÄ±zda olabildiÄŸince erken ekleyin. Verileriniz vektÃ¶r deposunda saklandÄ±ktan sonra eklemeye Ã§alÄ±ÅŸmayÄ±n.\n",
    "\n",
    "ğŸ‘‰ `embed_and_store()` fonksiyonunuzu uyarlayÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_and_store(\n",
    "    file_path: str,\n",
    "    vector_store: Chroma,\n",
    "    session_date: str | None = None,\n",
    "    chunk_size: int = 1200,\n",
    "    chunk_overlap: int = 150,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load a PDF, split into chunks, add metadata, store in Chroma.\n",
    "    Returns: list of stored document ids.\n",
    "    \"\"\"\n",
    "    if session_date is None:\n",
    "        session_date = str(date.today())\n",
    "\n",
    "    # 1) Load PDF (page-level docs)\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    docs = loader.load()  # list[Document], metadata includes \"source\" and \"page\" usually\n",
    "\n",
    "    # 2) Split into chunks\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    )\n",
    "    splits = splitter.split_documents(docs)\n",
    "\n",
    "    # 3) Add metadata EARLY (before storing)\n",
    "    for i, d in enumerate(splits):\n",
    "        d.metadata[\"session_date\"] = session_date\n",
    "        d.metadata[\"chunk_id\"] = i\n",
    "\n",
    "        # kaynaÄŸÄ± garantiye al\n",
    "        d.metadata[\"source\"] = d.metadata.get(\"source\", file_path)\n",
    "\n",
    "        # page alanÄ± Ã§oÄŸu zaman loader'dan gelir; yoksa -1 koy\n",
    "        d.metadata[\"page\"] = d.metadata.get(\"page\", -1)\n",
    "\n",
    "    # 4) Store\n",
    "    ids = vector_store.add_documents(splits)\n",
    "\n",
    "    # âŒ artÄ±k gerek yok (Chroma otomatik persist ediyor)\n",
    "    # vector_store.persist()\n",
    "\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "ğŸ‘‰ Fonksiyonunuzu deneyin ve vektÃ¶r deponuzun ek meta veri iÃ§erdiÄŸini kontrol edin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "def answer(\n",
    "    query: str,\n",
    "    vector_store: Chroma,\n",
    "    llm,\n",
    "    k: int = 6,\n",
    "    session_date: str | None = None,\n",
    "    max_chars_per_doc: int = 1500,\n",
    "):\n",
    "    \"\"\"\n",
    "    Query Chroma; optionally filter by session_date metadata.\n",
    "    Returns model output text.\n",
    "    \"\"\"\n",
    "    where = {\"session_date\": session_date} if session_date else None\n",
    "\n",
    "    retrieved_docs = vector_store.similarity_search(\n",
    "        query,\n",
    "        k=k,\n",
    "        filter=where,   # <- meta filtre burada\n",
    "    )\n",
    "\n",
    "    # context'i kontrollÃ¼ bÃ¼yÃ¼t (token ÅŸiÅŸmesini azaltÄ±r)\n",
    "    context = \"\\n\\n\".join(d.page_content[:max_chars_per_doc] for d in retrieved_docs)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a question-answering assistant.\n",
    "Answer using ONLY the CONTEXT below.\n",
    "\n",
    "Return:\n",
    "1) Answer (short)\n",
    "2) Evidence: quote 1â€“2 short snippets from the context that support the answer.\n",
    "\n",
    "If the answer is not in the context, say: \"I don't know based on the provided context.\"\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{query}\n",
    "\n",
    "FORMAT:\n",
    "Answer: ...\n",
    "Evidence:\n",
    "- \"...\"\n",
    "- \"...\"\n",
    "\"\"\".strip()\n",
    "\n",
    "    resp = llm.invoke(prompt)\n",
    "    return resp.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "Åimdi alÄ±cÄ±yÄ± kullanÄ±cÄ±nÄ±n sorduÄŸu tarihe gÃ¶re sÄ±nÄ±rlamamÄ±z gerekiyor.\n",
    "\n",
    "ğŸ‘‰ `answer()` fonksiyonunuzu bir tarih alabilecek ve yeni meta verilere dayalÄ± olarak belgeleri filtreleyebilecek ÅŸekilde uyarlayÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "ids = embed_and_store(\n",
    "    file_path=\"data/CRE-10-2026-01-27_EN.pdf\",\n",
    "    vector_store=vector_store,\n",
    "    session_date=\"2026-01-27\",\n",
    ")\n",
    "\n",
    "len(ids), ids[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "query = \"What are the main topics discussed in this session?\"\n",
    "print(answer(query, vector_store, llm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the main topics discussed in this session?\"\n",
    "print(answer(query, vector_store, llm, session_date=\"2026-01-27\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "Harika! GÃ¼Ã§lÃ¼ bir RAG sistemi oluÅŸturmak iÃ§in benzerlik aramasÄ±nÄ± meta veri aramasÄ±yla birleÅŸtirdiniz!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workintech_current",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
